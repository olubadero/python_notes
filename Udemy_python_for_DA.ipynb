{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abfa1632",
   "metadata": {},
   "source": [
    "## Introduction to Numpy\n",
    "\n",
    "#### Lec 7 - Creating Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35894870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Numpy arrays\n",
    "import numpy as np\n",
    "\n",
    "# Converting from a list\n",
    "#Lets start with a list\n",
    "\n",
    "my_list1 = [1,2,3,4]\n",
    "\n",
    "my_array1 = np.array(my_list1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e9274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out array\n",
    "\n",
    "my_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a407021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make another list\n",
    "my_list2 = [11,22,33,44]\n",
    "\n",
    "#Make a list of lists\n",
    "my_lists = [my_list1,my_list2]\n",
    "\n",
    "#Make multi-dimensional array\n",
    "my_array2 = np.array(my_lists)\n",
    "\n",
    "#Show array\n",
    "my_array2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc889adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Lets get the size of the array\n",
    "my_array2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find out the data tyoe of the array\n",
    "my_array2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d264c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making special case arrays\n",
    "\n",
    "#Zeros\n",
    "np.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf70bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Ones\n",
    "np.ones((5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1756277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An empty array\n",
    "\n",
    "np.empty(5)\n",
    "np.empty((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b033b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Identity array\n",
    "np.eye(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1815a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a range\n",
    "\n",
    "np.arange(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c167dfd7",
   "metadata": {},
   "source": [
    "#### Lec 8 - Using arrays and scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f64e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "5/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067cc38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes care of floats\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8db0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array\n",
    "arr1 = np.array([[1,2,3],[8,9,10]])\n",
    "\n",
    "#Show\n",
    "arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017a56a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiplying Arrays\n",
    "arr1*arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f8350",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Subtraction\n",
    "arr1-arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5367a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Arithmetic operations with scalars on array\n",
    "1 / arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69da671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exponential operation\n",
    "arr1 ** 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e15595",
   "metadata": {},
   "source": [
    "#### Lec 9 -Indexing Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b8bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Creating sample array\n",
    "arr = np.arange(0,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0714b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Get a value at an index\n",
    "arr[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e48dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get values in a range\n",
    "arr[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d9d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get values in a range\n",
    "arr[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad385b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting a value with index range (Broadcasting)\n",
    "arr[0:5]=100\n",
    "\n",
    "#Show\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d93a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Reset array, we'll see why i had to reset in  a moment\n",
    "arr = np.arange(0,11)\n",
    "\n",
    "#Show\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd56460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important notes on Slices\n",
    "slice_of_arr = arr[0:6]\n",
    "\n",
    "#Show slice\n",
    "slice_of_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251cd4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Change Slice\n",
    "slice_of_arr[:]=99\n",
    "\n",
    "#Show Slice again\n",
    "slice_of_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d77cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now note the changes also occur in our original array!\n",
    "arr\n",
    "\n",
    "# Data is not copied, it's a view of the original array! This avoids memory problems!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e360c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get a copy, need to be explicit\n",
    "arr_copy = arr.copy()\n",
    "\n",
    "arr_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f469be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing a 2D array\n",
    "\n",
    "arr_2d = np.array(([5,10,15],[20,25,30],[35,40,45]))\n",
    "\n",
    "#Show\n",
    "arr_2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e789afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing row\n",
    "arr_2d[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d38bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format is arr_2d[row][col] or arr_2d[row,col]\n",
    "\n",
    "# Getting individual element value\n",
    "arr_2d[1][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d74eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Getting individual element value\n",
    "arr_2d[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa21ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D array slicing\n",
    "\n",
    "#Shape (2,2) from top right corner\n",
    "arr_2d[:2,1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2558a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Shape bottom row\n",
    "arr_2d[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape bottom row\n",
    "arr_2d[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943ad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fancy Indexing\n",
    "\n",
    "#Set up matrix\n",
    "arr2d = np.zeros((10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8edcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Length of array\n",
    "arr_length = arr2d.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c2fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up array\n",
    "\n",
    "for i in range(arr_length):\n",
    "    arr2d[i] = i\n",
    "    \n",
    "arr2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f99bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Fancy indexing allows the following\n",
    "arr2d[[2,4,6,8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc70be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Allows in any order\n",
    "arr2d[[6,4,2,7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f285aeb",
   "metadata": {},
   "source": [
    "#### Lec 10 - Array Transposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create array\n",
    "arr = np.arange(50).reshape((10,5))\n",
    "\n",
    "#Show\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115c6e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets transpose\n",
    "arr.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1638500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking dot product of matrices\n",
    "np.dot(arr.T,arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a7895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 3D matrix\n",
    "arr3d = np.arange(50).reshape((5,5,2))\n",
    "\n",
    "#Show\n",
    "arr3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also transpose a 3d matrix\n",
    "\n",
    "arr3d.transpose((1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3615246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you need to get more specific use swapaxes\n",
    "arr = np.array([[1,2,3]])\n",
    "\n",
    "#Show \n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd04bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.swapaxes(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b6812d",
   "metadata": {},
   "source": [
    "#### Lec 11 - Universal Array Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39663b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.arange(11)\n",
    "\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad607d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking Square Roots\n",
    "np.sqrt(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd1d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Calcualting exponential (e^)\n",
    "np.exp(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fbbd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Functions require two arrays\n",
    "\n",
    "#Random array (normal dist)\n",
    "A = np.random.randn(10)\n",
    "\n",
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45c4bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random array (normal dist)\n",
    "B = np.random.randn(10)\n",
    "B\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ac49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Addition\n",
    "np.add(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d6950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding max or min between two arrays\n",
    "np.maximum(A,B)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397ea542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For full and extensive list of all universal functions\n",
    "website = \"http://docs.scipy.org/doc/numpy/reference/ufuncs.html#available-ufuncs\"\n",
    "import webbrowser\n",
    "webbrowser.open(website)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678124bc",
   "metadata": {},
   "source": [
    "#### Lec 12 - Array Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceaf435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b557046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set array for one side of grid\n",
    "points = np.arange(-5,5,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf76275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the grid\n",
    "dx,dy=np.meshgrid(points,points)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9483de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Show what one side looks like\n",
    "dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21a88a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Function\n",
    "z = (np.sin(dx) + np.sin(dy))\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1e8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot out the 2d array\n",
    "plt.imshow(z)\n",
    "\n",
    "#Plot with a colorbar\n",
    "plt.colorbar()\n",
    "\n",
    "#Give the plot a title\n",
    "plt.title(\"Plot for sin(x)+sin(y)\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee43fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets learn how to use the numpy where\n",
    "\n",
    "#First the slow way to do things\n",
    "\n",
    "A = np.array([1,2,3,4])\n",
    "\n",
    "B= np.array([100,200,300,400])\n",
    "\n",
    "#Now a boolean array\n",
    "condition = np.array([True,True,False,False])\n",
    "\n",
    "#Using a list comprehension\n",
    "answer = [(A_val if cond else B_val) for A_val,B_val,cond in zip(A,B,condition)]\n",
    "\n",
    "#Show the answer\n",
    "answer\n",
    "\n",
    "#Problems include speed issues and multi-dimensional array issues\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c663089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now using numpy.where\n",
    "\n",
    "answer2 = np.where(condition,A,B)\n",
    "\n",
    "#Show\n",
    "answer2\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caa9596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can use np.where  on 2d for manipulation\n",
    "\n",
    "from numpy.random import randn\n",
    "\n",
    "arr = randn(5,5)\n",
    "\n",
    "#Show arr\n",
    "arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acaedac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where array is less than zero, make that value zero, otherwise leave it as the array value\n",
    "np.where(arr < 0,0,arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0efff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other Statistical Processing\n",
    "arr = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f28d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUM\n",
    "arr.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9432788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Can also do along an axis (we shold expect a 3 diff between the columns)\n",
    "arr.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad79f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Mean\n",
    "arr.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab615bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Deviation\n",
    "arr.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce069301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variance\n",
    "arr.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0432829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Also any and all for processing boolean arrays\n",
    "\n",
    "bool_arr = np.array([True,False,True])\n",
    "\n",
    "#For any True\n",
    "bool_arr.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149907b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For all True\n",
    "bool_arr.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24519424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally sort array\n",
    "\n",
    "#Create a random array\n",
    "arr = randn(5)\n",
    "#show\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d56e0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort it\n",
    "arr.sort()\n",
    "#show\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32105735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets learn about unique\n",
    "countries = np.array(['France', 'Germany', 'USA', 'Russia','USA','Mexico','Germany'])\n",
    "\n",
    "np.unique(countries)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd38583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in1d test values in one array\n",
    "np.in1d(['France','USA','Sweden'],countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd03c13",
   "metadata": {},
   "source": [
    "#### Lec 13 - Array Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an array\n",
    "arr = np.arange(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2389c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Saving array on disk in binary format (file extension .npy)\n",
    "np.save('my_array',arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc42131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change arr\n",
    "arr = np.arange(10)\n",
    "#Show\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baecaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see the original saved copy\n",
    "np.load('my_array.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42740e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Saving multiple arrays into a zip file\n",
    "np.savez('two_arrays.npz',x=arr,y=arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a65c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now loading multiple arrays\n",
    "archive_array = np.load('two_arrays.npz')\n",
    "\n",
    "#Show\n",
    "archive_array['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f69184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets remove them from the memory\n",
    "rm my_array.npy\n",
    "rm two_arrays.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a0bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Now saving and loading text files\n",
    "\n",
    "arr = np.array([[1,2,3],[4,5,6]])\n",
    "np.savetxt('my_test_text.txt',arr,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74821a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arr = np.loadtxt('my_test_text.txt',delimiter = ',')\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8ee13d",
   "metadata": {},
   "source": [
    "#### Lec 14 - Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec98ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc2deb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create a Series (array of data and data labels, its index)\n",
    "\n",
    "obj = Series([3,6,9,12])\n",
    "\n",
    "#Show\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d505b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets show the values\n",
    "obj.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aaea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Lets show the index\n",
    "obj.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f15955",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets create a Series with an index\n",
    "\n",
    "#WW2 casualties \n",
    "ww2_cas = Series([8700000,4300000,3000000,2100000,400000],index=['USSR','Germany','China','Japan','USA'])\n",
    "\n",
    "#Show\n",
    "ww2_cas\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can use index values to select Series values\n",
    "ww2_cas['USA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b7599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can also check with array operations\n",
    "\n",
    "#Check who had casualties greater than 4 million\n",
    "ww2_cas[ww2_cas>4000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a6a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can treat Series as ordered dictionary\n",
    "\n",
    "#Check if USSR is in Series\n",
    "'USSR' in ww2_cas\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e499cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can convert Series into Python dictionary\n",
    "ww2_dict = ww2_cas.to_dict()\n",
    "\n",
    "#Show\n",
    "ww2_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22bb4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can convert back into a Series\n",
    "WW2_Series = Series(ww2_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988704cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show\n",
    "WW2_Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a17fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passing a dictionary the index will have the dict keys in order\n",
    "countries = ['China','Germany','Japan','USA','USSR','Argentina']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c161e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets redefine a Series\n",
    "obj2 = Series(ww2_dict,index=countries)\n",
    "     \n",
    "\n",
    "#Show\n",
    "obj2\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use isnull and notnull to find missing data\n",
    "pd.isnull(obj2)\n",
    "\n",
    "#obj2.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee3ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same for the opposite\n",
    "pd.notnull(obj2)\n",
    "\n",
    "#obj2.notnull()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1088255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see the ww2 Series again\n",
    "WW2_Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe6da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check our Series with Argentine again\n",
    "obj2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can add and pandas automatically aligns data by index\n",
    "WW2_Series + obj2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c2522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can give Series names\n",
    "obj2.name = \"World War 2 Casualties\"\n",
    "     \n",
    "#Show\n",
    "obj2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42de7ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also name index\n",
    "obj2.index.name = 'Countries'\n",
    "     \n",
    "\n",
    "#Show\n",
    "obj2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bbf29f",
   "metadata": {},
   "source": [
    "\n",
    "#### Next we'll learn DataFrames!\n",
    "\n",
    "#### Lec 15 - DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we'll learn DataFrames\n",
    "\n",
    "#Let's get some data to play with. How about the NFL?\n",
    "import webbrowser\n",
    "website = 'http://en.wikipedia.org/wiki/NFL_win-loss_records'\n",
    "webbrowser.open(website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce1e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy and read to get data\n",
    "nfl_frame = pd.read_clipboard()\n",
    "\n",
    "#Show\n",
    "nfl_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e846d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can grab the oclumn names with .columns\n",
    "nfl_frame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c51d93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see some specific data columns\n",
    "DataFrame(nfl_frame,columns=['Team','First Season','Total Games'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What happens if we ask for a column that doesn't exist?\n",
    "DataFrame(nfl_frame,columns=['Team','First Season','Total Games','Stadium'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cdae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call columns\n",
    "nfl_frame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd41514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can retrieve individual columns\n",
    "nfl_frame.Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or try this method for multiple word columns\n",
    "nfl_frame['Total Games']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69473b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can retrieve rows through indexing\n",
    "nfl_frame.ix[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6914f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also assign value sto entire columns\n",
    "nfl_frame['Stadium']=\"Levi's Stadium\" #Careful with the ' here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4678d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting numbers for stadiums\n",
    "nfl_frame[\"Stadium\"] = np.arange(5)\n",
    "\n",
    "#Show\n",
    "nfl_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe4ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call columns\n",
    "nfl_frame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fd6d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a Series to a DataFrame\n",
    "stadiums = Series([\"Levi's Stadium\",\"AT&T Stadium\"],index=[4,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now input into the nfl DataFrame\n",
    "nfl_frame['Stadium']=stadiums\n",
    "\n",
    "#Show\n",
    "nfl_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f569f7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also delete columns\n",
    "del nfl_frame['Stadium']\n",
    "\n",
    "nfl_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4700e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrames can be constructed many ways. Another way is from a dictionary of equal length lists\n",
    "data = {'City':['SF','LA','NYC'],\n",
    "        'Population':[837000,3880000,8400000]}\n",
    "\n",
    "city_frame = DataFrame(data)\n",
    "\n",
    "#Show\n",
    "city_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb771be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For full list of ways to create DataFrames from various sources go to teh documentation for pandas:\n",
    "website = 'http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.html'\n",
    "webbrowser.open(website)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a05744",
   "metadata": {},
   "source": [
    "#### Lec 16 - Index Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf108ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's learn/review about Index Objects\n",
    "my_ser = Series([1,2,3,4],index=['A','B','C','D'])\n",
    "\n",
    "#Get the index\n",
    "my_index = my_ser.index\n",
    "     \n",
    "\n",
    "#Show\n",
    "my_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f469e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can grab index ranges\n",
    "my_index[2:]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49265516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What happens if we try to change an index value?\n",
    "my_index[0] = 'Z'\n",
    "\n",
    "#Excellent! Indexes are immutable\n",
    "\n",
    "#Next we'll learn about Reindexing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb89c81c",
   "metadata": {},
   "source": [
    "#### Lec 17 -Reindexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e147b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "from numpy.random import randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac4d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create a new series\n",
    "ser1 = Series([1,2,3,4],index=['A','B','C','D'])\n",
    "     \n",
    "#Show\n",
    "ser1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a22fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call reindex to rearrange the data to a new index\n",
    "ser2 = ser1.reindex(['A','B','C','D','E','F'])\n",
    "     \n",
    "#Show\n",
    "ser2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can alos fill in values for new indexes\n",
    "ser2.reindex(['A','B','C','D','E','F','G'],fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bf1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a particular method for filling values\n",
    "ser3 = Series(['USA','Mexico','Canada'],index=[0,5,10])\n",
    "\n",
    "#Show\n",
    "ser3\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764dfb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can use a forward fill for interploating values vetween indices \n",
    "ser3.reindex(range(15),method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ee1351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can use a backward fill for interploating values vetween indices \n",
    "ser3.reindex(range(15),method='backfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b035a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reindexing rows, columns or both\n",
    "\n",
    "#Lets make a datafram ewith some random values\n",
    "dframe = DataFrame(randn(25).reshape((5,5)),index=['A','B','D','E','F'],columns=['col1','col2','col3','col4','col5'])\n",
    "\n",
    "#Show\n",
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9fdd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice we forgot 'C' , lets reindex it into dframe\n",
    "dframe2 = dframe.reindex(['A','B','C','D','E','F'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13642bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can also explicitly reindex columns\n",
    "new_columns = ['col1','col2','col3','col4','col5','col6']\n",
    "\n",
    "dframe2.reindex(columns=new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc41b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reindex quickly using the label-indexing with ix (we'll see this more in the future)\n",
    "\n",
    "#Show original\n",
    "dframe\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ac5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe.ix[['A','B','C','D','E','F'],new_columns]\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5afd9a",
   "metadata": {},
   "source": [
    "#### Lec 18 -Drop Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52136e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new series to play with\n",
    "ser1 = Series(np.arange(3),index=['a','b','c'])\n",
    "\n",
    "#Show\n",
    "ser1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69017c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's drop an index\n",
    "ser1.drop('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69262eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With a DataFrame we can drop values from either axis\n",
    "dframe1 = DataFrame(np.arange(9).reshape((3,3)),index=['SF','LA','NY'],columns=['pop','size','year'])\n",
    "\n",
    "#Show (remember just random values)\n",
    "dframe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7920298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now dropping a row\n",
    "dframe1.drop('LA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a9b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or we could drop a column\n",
    "\n",
    "#Need to specify that axis is 1, not 0\n",
    "dframe1.drop('year',axis=1)\n",
    "\n",
    "#Next we'll learn about selecting entires in a DataFrame!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8609c9d",
   "metadata": {},
   "source": [
    "#### Lec 19 - Selecting Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8547ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets try some Series indexing\n",
    "ser1 = Series(np.arange(3),index=['A','B','C'])\n",
    "\n",
    "#multiply all values by 2, to avoid confusion in future\n",
    "ser1 = 2*ser1\n",
    "\n",
    "#Show\n",
    "ser1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbdc0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can grab entry by index name\n",
    "ser1['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd61c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Or grab by index \n",
    "ser1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e285c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Can also grab by index range\n",
    "ser1[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf887337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or grab range by range of index values\n",
    "ser1[['A','B','C']]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa04eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or grab by logic\n",
    "ser1[ser1>3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc68ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can also ser using these methods\n",
    "ser1[ser1>3] = 10\n",
    "\n",
    "#Show\n",
    "ser1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba02ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's see sleection in a DataFrame\n",
    "\n",
    "dframe = DataFrame(np.arange(25).reshape((5,5)),index=['NYC','LA','SF','DC','Chi'],columns=['A','B','C','D','E'])\n",
    "\n",
    "#Show\n",
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc02115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select by column name\n",
    "dframe['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17410c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select by multiple columns\n",
    "dframe[['B','E']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a5a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can also use boolean\n",
    "dframe[dframe['C']>8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can also just shoe a boolean DataFrame\n",
    "dframe> 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can alos use ix as previously discussed to label-index\n",
    "dframe.ix['LA']\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d726c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another example\n",
    "dframe.ix[1]\n",
    "\n",
    "#Next we'll learn about data alignment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d3963",
   "metadata": {},
   "source": [
    "#### Lec 20 - Data Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets start by making two Series\n",
    "\n",
    "ser1 = Series([0,1,2],index=['A','B','C'])\n",
    "\n",
    "#Show\n",
    "ser1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843011f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now second Series 2\n",
    "ser2 = Series([3,4,5,6],index=['A','B','C','D'])\n",
    "\n",
    "#Show \n",
    "ser2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04675110",
   "metadata": {},
   "outputs": [],
   "source": [
    "#So what happens when we add these together\n",
    "ser1 + ser2\n",
    "\n",
    "#Note the NaN values are added in automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5a7a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now let's try it with DataFrames!\n",
    "dframe1 = DataFrame(np.arange(4).reshape(2,2),columns=list('AB'),index=['NYC','LA'])\n",
    "\n",
    "#Show\n",
    "dframe1\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c599d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second DataFrame\n",
    "dframe2 = DataFrame(np.arange(9).reshape(3,3),columns=list('ADC'),index=['NYC','SF','LA'])\n",
    "\n",
    "#Show\n",
    "dframe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47093df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What happens when we add them together?\n",
    "\n",
    "dframe1 + dframe2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99c24fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What if we want to replace the NaN values\n",
    "# Then we can use .add()\n",
    "\n",
    "dframe1.add(dframe2,fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a3c153",
   "metadata": {},
   "source": [
    "Now we can see that the values are filled, however there was no SF,B value so that is still NaN\n",
    "\n",
    "Lets learn about operations betwen a Series and a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show\n",
    "dframe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb2d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Series from DataFrame's 0 row\n",
    "ser3 = dframe2.ix[0]\n",
    "\n",
    "#Show\n",
    "ser3\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd643b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can use arithmetic operations\n",
    "dframe2-ser3\n",
    "\n",
    "#Next we'll learn about sorting and ranking!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa332a8f",
   "metadata": {},
   "source": [
    "#### Lec 21 - Rank and Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7386bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting by index\n",
    "ser1 = Series(range(3),index=['C','A','B'])\n",
    "\n",
    "#show\n",
    "ser1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb2574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now sort_index\n",
    "ser1.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b834d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Can sort a Series by its values\n",
    "ser1.order()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb2d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see how ranking works\n",
    "\n",
    "from numpy.random import randn\n",
    "ser2 = Series(randn(10))\n",
    "\n",
    "#Show\n",
    "ser2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e1a991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will show you the rank used if you sort the series\n",
    "ser2.rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743051e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets sort it now\n",
    "ser2.sort()\n",
    "\n",
    "#Show\n",
    "ser2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f061bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After sorting let's check the rank and see iof it makes sense\n",
    "ser2.rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b526f1",
   "metadata": {},
   "source": [
    "On the left column we see th original index value and on the right we see it's rank!\n",
    "Next we'll learn about using descriptive statistics on dataframes!\n",
    "\n",
    "#### Lec 22 - Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f2327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create a dataframe to work with\n",
    "arr = np.array([[1,2,np.nan],[np.nan,3,4]])\n",
    "dframe1 = DataFrame(arr,index=['A','B'],columns = ['One','Two','Three'])\n",
    "\n",
    "#Show\n",
    "dframe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9956efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the sum() method in action\n",
    "dframe1.sum()\n",
    "\n",
    "#Notice how it ignores NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82410de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#We can also over rows instead of columns\n",
    "dframe1.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac1092",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can also grab min and max values of dataframe\n",
    "dframe1.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95efb564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As well as there index\n",
    "dframe1.idxmin()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c1346",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same deal with max, just replace min for max\n",
    "dframe1.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39713206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show\n",
    "dframe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b25b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can also do an accumulation sum\n",
    "dframe1.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547f0ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A very useful feature is describe, which provides summary statistics\n",
    "dframe1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efd065a",
   "metadata": {},
   "source": [
    "We can also get information on correlation and covariance\n",
    "\n",
    "For more info on correlation and covariance, check out the videos below!\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a695deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "# For more information about Covariaance and Correlation\n",
    "# Check out these great videos!\n",
    "# Video credit: Brandon Foltz.\n",
    "\n",
    "#CoVariance\n",
    "YouTubeVideo('xGbpuFNR1ME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab82813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation\n",
    "YouTubeVideo('4EXNedimDMs')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets check correlation and covariance on some stock prices!\n",
    "\n",
    "#Pandas can get info off the web\n",
    "import pandas.io.data as pdweb\n",
    "\n",
    "#Set datetime for date input\n",
    "import datetime\n",
    "\n",
    "#Get the closing prices\n",
    "\n",
    "prices = pdweb.get_data_yahoo(['CVX','XOM','BP'], \n",
    "                               start=datetime.datetime(2010, 1, 1), \n",
    "                               end=datetime.datetime(2013, 1, 1))['Adj Close']\n",
    "#Show preview\n",
    "prices.head()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets get the volume trades\n",
    "\n",
    "volume = pdweb.get_data_yahoo(['CVX','XOM','BP'], \n",
    "                               start=datetime.datetime(2010, 1, 1), \n",
    "                               end=datetime.datetime(2013, 1, 1))['Volume']\n",
    "\n",
    "#Show preview\n",
    "volume.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2179cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets get the return\n",
    "rets = prices.pct_change()\n",
    "     \n",
    "\n",
    "#Get the correlation of the stocks\n",
    "corr = rets.corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc30f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see the prices over time to get a very rough idea of the correlation between the stock prices\n",
    "prices.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e1a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#As expected pretty strong correlations with eachother\n",
    "sns.heatmap(rets.corr())\n",
    "\n",
    "#We'll learn much more about seaborn later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f367da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We can also check for unique values and their counts \n",
    "\n",
    "#For example\n",
    "ser1 = Series(['w','w','x', 'y', 'z' ,'w' ,'w' ,'x' ,'x' ,'y' ,'a' ,'z' ])\n",
    "\n",
    "#Show\n",
    "ser1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb49926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab the unique values\n",
    "ser1.unique()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c2a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now get the count of the unique values\n",
    "ser1.value_counts()\n",
    "     \n",
    "#Next we'll learn how to best deal with missing data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324d872",
   "metadata": {},
   "source": [
    "#### Lec 23 - Missing Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591756e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we'll learn how to deal with missing data, a very common task when analyzing datasets!\n",
    "\n",
    "data = Series(['one','two', np.nan, 'four'])\n",
    "     \n",
    "\n",
    "#Show data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd91ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the missing values\n",
    "data.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f667bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can simply drop the NAN \n",
    "data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851cf1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a DataFrame we need to be a little more careful!\n",
    "\n",
    "dframe = DataFrame([[1,2,3],[np.nan,5,6],[7,np.nan,9],[np.nan,np.nan,np.nan]])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58473aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show\n",
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dframe = dframe.dropna()\n",
    "     \n",
    "\n",
    "#Show\n",
    "clean_dframe\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed5df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note all rows where an NA occured was a drop of the entire row\n",
    "     \n",
    "\n",
    "#We can also specify to only drop rows that are complete missing all data\n",
    "dframe.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b21ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or we can specify to drop columns with missing data\n",
    "dframe.dropna(axis=1)\n",
    "\n",
    "#This should drop all columns out since every column contains at least 1 NAN\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e10d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also threshold teh missing data as well\n",
    "\n",
    "#For example if we only want rows with at least 3 data points\n",
    "dframe2 = DataFrame([[1,2,3,np.nan],[2,np.nan,5,6],[np.nan,7,np.nan,9],[1,np.nan,np.nan,np.nan]])\n",
    "\n",
    "#Show\n",
    "dframe2\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e354523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Droppin any rows tht dont have at least 2 data points\n",
    "dframe2.dropna(thresh=2)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf07e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropiing rows without at least 3 data points\n",
    "dframe2.dropna(thresh=3)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also fill any NAN\n",
    "dframe2.fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d927a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can also fill in diff values for diff columns\n",
    "dframe2.fillna({0:0,1:1,2:2,3:3})\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b5590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that we still have access to the original dframe\n",
    "dframe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c73fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we want to modify the exsisting object, use inplace\n",
    "dframe2.fillna(0,inplace=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e72b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's see the dframe\n",
    "dframe2\n",
    "\n",
    "#Awesome! Next we'll learn about Index Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ec46a",
   "metadata": {},
   "source": [
    "#### Lec 24 - Index Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fad65b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3415c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we'll learn about Index Hierarchy\n",
    "\n",
    "#pandas allows you to have multiple index levels, which is very clear with this example:\n",
    "\n",
    "ser = Series(np.random.randn(6),index=[[1,1,1,2,2,2],['a','b','c','a','b','c']])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8827a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show Series with multiple index levels\n",
    "ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33603374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check the multiple levels\n",
    "ser.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f8cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can sleect specific subsets\n",
    "ser[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07990919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also select from an internal index level\n",
    "ser[:,'a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a367c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also create Data Frames from Series with multiple levels\n",
    "dframe = ser.unstack()\n",
    "\n",
    "#Show\n",
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706844dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can also reverse\n",
    "dframe.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa114525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also apply multiple level indexing to DataFrames\n",
    "dframe2 = DataFrame(np.arange(16).reshape(4,4),\n",
    "                    index=[['a','a','b','b'],[1,2,1,2]],\n",
    "                    columns=[['NY','NY','LA','SF'],['cold','hot','hot','cold']])\n",
    "                                                   \n",
    "dframe2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f66d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also give these index levels names\n",
    "\n",
    "#Name the index levels\n",
    "dframe2.index.names = ['INDEX_1','INDEX_2']\n",
    "\n",
    "#Name the column levels\n",
    "dframe2.columns.names = ['Cities','Temp']\n",
    "\n",
    "dframe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eff9c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also interchange level orders (note the axis=1 for columns)\n",
    "dframe2.swaplevel('Cities','Temp',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c63970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also sort levels\n",
    "dframe2.sortlevel(1)\n",
    "\n",
    "#Note the change in sorting, now the Dframe index is sorted by the INDEX_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45634488",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#We can also perform operations on particular levels\n",
    "dframe2.sum(level='Temp',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad10ee70",
   "metadata": {},
   "source": [
    "#### Lec 25 - Reading and Writing Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751edc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can open csv files as a dataframe\n",
    "dframe = pd.read_csv('lec25.csv')\n",
    "\n",
    "#Show\n",
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e9756b",
   "metadata": {},
   "outputs": [],
   "source": [
    " #If we dont want the header to be the first row\n",
    "dframe = pd.read_csv('lec25.csv',header=None)\n",
    "\n",
    "#Show\n",
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2b3b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also indicate a particular number of rows to be read\n",
    "pd.read_csv('lec25.csv',header=None,nrows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f88bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's see dframe again\n",
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec1f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see how we can write DataFrames out to text files\n",
    "dframe.to_csv('mytextdata_out.csv')\n",
    "\n",
    "#You'll see this file where you're ipython Notebooks are saved (Usually under my documents)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da9ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We can also use other delimiters\n",
    "\n",
    "#we'll import sys to see the output\n",
    "import sys \n",
    "\n",
    "#Use sys.stdout to see the output directly and not save it\n",
    "dframe.to_csv(sys.stdout,sep='_')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56539245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to make sure we understand the delimiter\n",
    "dframe.to_csv(sys.stdout,sep='?')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f811d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also choose to write only a specific subset of columns\n",
    "dframe.to_csv(sys.stdout,columns=[0,1,2])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35443a0",
   "metadata": {},
   "source": [
    "#You should also check out pythons built-in csv reader and writer for more info:\n",
    " https://docs.python.org/2/library/csv.html\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d8e22",
   "metadata": {},
   "source": [
    "#### Lec 26 - JSON with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b15ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heres an example of what a JSON (JavaScript Object Notation) looks like:\n",
    "json_obj = \"\"\"\n",
    "{   \"zoo_animal\": \"Lion\",\n",
    "    \"food\": [\"Meat\", \"Veggies\", \"Honey\"],\n",
    "    \"fur\": \"Golden\",\n",
    "    \"clothes\": null, \n",
    "    \"diet\": [{\"zoo_animal\": \"Gazelle\", \"food\":\"grass\", \"fur\": \"Brown\"}]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fba04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let import json module\n",
    "import json\n",
    "\n",
    "#Lets load json data\n",
    "data = json.loads(json_obj)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd2e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#WE can also convert back to JSON\n",
    "json.dumps(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd6e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can simply open JSON data after loading with a DataFrame\n",
    "dframe = DataFrame(data['diet'])\n",
    "     \n",
    "\n",
    "#Show\n",
    "dframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca27072",
   "metadata": {},
   "source": [
    "#### Lec 27 - HTML with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5120c49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets grab a url for list of failed banks\n",
    "url = 'http://www.fdic.gov/bank/individual/failed/banklist.html'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac9236",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "IMPORTANT NOTE: NEED TO HAVE beautiful-soup INSTALLED as well as html5lib !!!!\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74179c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab data from html and put it intop a list of DataFrame objects!\n",
    "dframe_list = pd.io.html.read_html(url)\n",
    "     \n",
    "\n",
    "#Grab the first list item from the data base and set as a DataFrame\n",
    "dframe = dframe_list[0]\n",
    "     \n",
    "\n",
    "#Show\n",
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c47f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243160a7",
   "metadata": {},
   "source": [
    "#### Lec 28 - Excel with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96076fa6",
   "metadata": {},
   "source": [
    "#Now we'll learn how to work with excel files\n",
    "     \n",
    "\n",
    "\"\"\"\n",
    "IMPORTANT NOTE: NEED TO HAVE xlrd AND openpyxl INSTALLED!!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99128acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the excel file as an object\n",
    "xlsfile = pd.ExcelFile('Lec_28_test.xlsx')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e05661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the first sheet of the excel file and set as DataFrame\n",
    "dframe = xlsfile.parse('Sheet1')\n",
    "     \n",
    "\n",
    "#Show!\n",
    "dframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77da73d",
   "metadata": {},
   "source": [
    "#### Lec 29 - Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f5b5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a dframe\n",
    "\n",
    "dframe1 = DataFrame({'key':['X','Z','Y','Z','X','X'],'data_set_1': np.arange(6)})\n",
    "\n",
    "#Show\n",
    "dframe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d2ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets make another dframe\n",
    "\n",
    "dframe2 = DataFrame({'key':['Q','Y','Z'],'data_set_2':[1,2,3]})\n",
    "\n",
    "#Show\n",
    "dframe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d04eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use merge the dataframes, this is a \"many-to-one\" situation\n",
    "\n",
    "# Merge will automatically choose overlapping columns to merge on\n",
    "pd.merge(dframe1,dframe2)\n",
    "\n",
    "#Note no overlapping 'X's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd3bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could have also specified which column to merge on\n",
    "pd.merge(dframe1,dframe2,on='key')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a314e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can choose which DataFrame's keys to use, this will choose left (dframe1)\n",
    "pd.merge(dframe1,dframe2,on='key',how='left')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab22b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the one on the right (dframe2)\n",
    "pd.merge(dframe1,dframe2,on='key',how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2090c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing the \"outer\" method selects the union of both keys\n",
    "pd.merge(dframe1,dframe2,on='key',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f661552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we'll learn about a many to many merge\n",
    "\n",
    "# Nnote that these DataFrames contain more than one instance of the key in BOTH datasets\n",
    "\n",
    "dframe3 = DataFrame({'key': ['X', 'X', 'X', 'Y', 'Z', 'Z'],\n",
    "                 'data_set_3': range(6)})\n",
    "dframe4 = DataFrame({'key': ['Y', 'Y', 'X', 'X', 'Z'],\n",
    "                 'data_set_4': range(5)})\n",
    "\n",
    "#Show the merge\n",
    "pd.merge(dframe3, dframe4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f51e8",
   "metadata": {},
   "source": [
    "So what happened? A many to many merge results in the product of the rows. Because there were 3 'X's in dframe3 and 2 'X's in dframe4 there ended up being a total of 6 'X' rows in the result (2*3=6)! Note how dframe3 repeats its 0,1,2 values for 'X' and dframe4 repeats its '2,3' pairs throughout the key set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6561b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also merge with multiple keys!\n",
    "\n",
    "# Dframe on left\n",
    "df_left = DataFrame({'key1': ['SF', 'SF', 'LA'],\n",
    "                  'key2': ['one', 'two', 'one'],\n",
    "                  'left_data': [10,20,30]})\n",
    "\n",
    "#Dframe on right\n",
    "df_right = DataFrame({'key1': ['SF', 'SF', 'LA', 'LA'],\n",
    "                   'key2': ['one', 'one', 'one', 'two'],\n",
    "                   'right_data': [40,50,60,70]})\n",
    "\n",
    "#Merge\n",
    "pd.merge(df_left, df_right, on=['key1', 'key2'], how='outer')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9286328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now using the above you can check mulitple data sets for multiple key combos, for instance what did the left data set have for LA,one?\n",
    "# Answer =  60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01305b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Note that the left and right DataFrames have overlapping key names (key1 and key2).\n",
    "# pandas automatically adds suffixes to them\n",
    "\n",
    "pd.merge(df_left,df_right,on='key1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8f11a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also specify what the suffix becomes\n",
    "pd.merge(df_left,df_right, on='key1',suffixes=('_lefty','_righty'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df94c1a",
   "metadata": {},
   "source": [
    " For more info on merge parameters check out:\n",
    "url = 'http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.merge.html'\n",
    "\n",
    "Next we'll learn how to merge on Index!\n",
    "\n",
    "\n",
    "#### Lec 30 -Merge on Index.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7901b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get two dframes\n",
    "\n",
    "df_left = DataFrame({'key': ['X','Y','Z','X','Y'],\n",
    "                  'data': range(5)})\n",
    "df_right = DataFrame({'group_data': [10, 20]}, index=['X', 'Y'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb06ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Show\n",
    "df_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f270b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show\n",
    "df_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d99a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also get a union by using outer\n",
    "pd.merge(df_left,df_right,left_on='key',right_index=True,how='outer')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6580f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now merge, we'll use the key for the left Dframe, and the index for the right\n",
    "pd.merge(df_left,df_right,left_on='key',right_index=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697be5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's try something a little more complicated, remember hierarchal index?\n",
    "df_left_hr = DataFrame({'key1': ['SF','SF','SF','LA','LA'],\n",
    "                   'key2': [10, 20, 30, 20, 30],\n",
    "                   'data_set': np.arange(5.)})\n",
    "df_right_hr = DataFrame(np.arange(10).reshape((5, 2)),\n",
    "                   index=[['LA','LA','SF','SF','SF'],\n",
    "                          [20, 10, 10, 10, 20]],\n",
    "                   columns=['col_1', 'col_2'])\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85492076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show, this has a index hierarchy\n",
    "df_right_hr\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93978044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can merge the left by using keys and the right by its index\n",
    "pd.merge(df_left_hr,df_right_hr,left_on=['key1','key2'],right_index=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec5b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can alo keep a union by choosing 'outer' method\n",
    "pd.merge(df_left_hr,df_right_hr,left_on=['key1','key2'],right_index=True,how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c5448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE can also you .join()\n",
    "\n",
    "# Shown on our first two DataFrames\n",
    "df_left.join(df_right)\n",
    "\n",
    "# Next we'll learn about the concatenate function!\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d857c0",
   "metadata": {},
   "source": [
    "#### Lec 31 - Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4859af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First in just Numpy\n",
    "     \n",
    "\n",
    "# Create a matrix \n",
    "arr1 = np.arange(9).reshape((3,3))\n",
    "\n",
    "# Show\n",
    "arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2732d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate along axis 1\n",
    "np.concatenate([arr1,arr1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see other axis options\n",
    "np.concatenate([arr1,arr1],axis=0)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda4a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see how this works in pandas\n",
    "     \n",
    "\n",
    "# Lets create two Series with no overlap\n",
    "ser1 =  Series([0,1,2],index=['T','U','V'])\n",
    "\n",
    "ser2 = Series([3,4],index=['X','Y'])\n",
    "\n",
    "#Now let use concat (default is axis=0)\n",
    "pd.concat([ser1,ser2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8158ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now passing along another axis will produce a DataFrame\n",
    "pd.concat([ser1,ser2],axis=1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can specify which specific axes to be used\n",
    "pd.concat([ser1,ser2],axis=1,join_axes=[['U','V','Y']])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets say we wanted to add markers.keys to the concatenation result\n",
    "\n",
    "# WE can do this with a hierarchical index\n",
    "pd.concat([ser1,ser2],keys=['cat1','cat2'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b89584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Along the axis=1 then these Keys become column headers\n",
    "pd.concat([ser1,ser2],axis=1,keys=['cat1','cat2'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9f95a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lastly, everything works similarly in DataFrames\n",
    "\n",
    "dframe1 = DataFrame(np.random.randn(4,3), columns=['X', 'Y', 'Z'])\n",
    "dframe2 = DataFrame(np.random.randn(3, 3), columns=['Y', 'Q', 'X'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441ca417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat on DataFrame\n",
    "pd.concat([dframe1,dframe2])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we dont care about the index info and just awnt to make a complete DataFrame, just use ignore_index\n",
    "pd.concat([dframe1,dframe2],ignore_index=True)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab1edc",
   "metadata": {},
   "source": [
    "For more info in documentation:\n",
    "url='http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html'\n",
    "     \n",
    "Next up: More on Combining DataFrames with Overlapping Indexes!\n",
    "\n",
    "#### Lec 32 - Combining DataFrames\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce69ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets make some Series to work with\n",
    "\n",
    "#First Series\n",
    "ser1 = Series([2,np.nan,4,np.nan,6,np.nan],\n",
    "           index=['Q','R','S','T','U','V'])\n",
    "\n",
    "#Second Series (based off length of ser1)\n",
    "ser2 = Series(np.arange(len(ser1), dtype=np.float64),\n",
    "           index=['Q','R','S','T','U','V'])\n",
    "\n",
    "ser2[-1] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b661481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ser1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45efa252",
   "metadata": {},
   "outputs": [],
   "source": [
    "ser2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27122f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's get a series where the value of ser1 is chosen if ser2 is NAN,otherwise let the value be ser1\n",
    "Series(np.where(pd.isnull(ser1),ser2,ser1),index=ser1.index)\n",
    " \n",
    "#Take a moment to really understand how the above worked\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c150b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can do the same thing simply by using combine_first with pandas\n",
    "ser1.combine_first(ser2)\n",
    "\n",
    "#This combines the Series values, choosing the values of the calling Series first, unless its a NAN\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b579ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets how this works on a DataFrame!\n",
    "     \n",
    "\n",
    "#Lets make some \n",
    "dframe_odds = DataFrame({'X': [1., np.nan, 3., np.nan],\n",
    "                     'Y': [np.nan, 5., np.nan, 7.],\n",
    "                     'Z': [np.nan, 9., np.nan, 11.]})\n",
    "dframe_evens = DataFrame({'X': [2., 4., np.nan, 6., 8.],\n",
    "                     'Y': [np.nan, 10., 12., 14., 16.]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66503c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show\n",
    "dframe_odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show\n",
    "dframe_evens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f59a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets combine using odds values first, unless theres a NAN, then put the evens values\n",
    "dframe_odds.combine_first(dframe_evens)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c77542",
   "metadata": {},
   "source": [
    "#### Lec 33 - Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca2f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see how stack and unstack work\n",
    "\n",
    "# Create DataFrame\n",
    "dframe1 = DataFrame(np.arange(8).reshape((2, 4)),\n",
    "                 index=pd.Index(['LA', 'SF'], name='city'),\n",
    "                 columns=pd.Index(['A', 'B', 'C','D'], name='letter'))\n",
    "#Show\n",
    "dframe1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205ecec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use stack to pivot the columns into the rows\n",
    "dframe_st = dframe1.stack()\n",
    "\n",
    "#Show\n",
    "dframe_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aec63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can always rearrange back into a DataFrame\n",
    "dframe_st.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c285fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can choose which level to unstack by\n",
    "dframe_st.unstack(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eeff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also by which name to unstack by\n",
    "dframe_st.unstack('letter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also by which name to unstack by\n",
    "dframe_st.unstack('city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87374a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how stack and unstack handle NAN\n",
    "\n",
    "#Make two series\n",
    "ser1 = Series([0, 1, 2], index=['Q', 'X', 'Y'])\n",
    "ser2 = Series([4, 5, 6], index=['X', 'Y', 'Z'])\n",
    "\n",
    "#Concat to make a dframe\n",
    "dframe = pd.concat([ser1, ser2], keys=['Alpha', 'Beta'])\n",
    "\n",
    "# Unstack resulting DataFrame\n",
    "dframe.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c8146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now stack will filter out NAN by default\n",
    "dframe.unstack().stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aafe52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF we dont want this we can set it to False\n",
    "dframe.unstack().stack(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6075cad4",
   "metadata": {},
   "source": [
    "#### Lec 34 - Pivoting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4765d639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create some data to play with:\n",
    "\n",
    "# Note: It is not necessary to understand how this dataset was made to understand this Lecture.\n",
    "\n",
    "#import pandas testing utility\n",
    "import pandas.util.testing as tm; tm.N = 3\n",
    "\n",
    "#Create a unpivoted function\n",
    "def unpivot(frame):\n",
    "    N, K = frame.shape\n",
    "    \n",
    "    data = {'value' : frame.values.ravel('F'),\n",
    "            'variable' : np.asarray(frame.columns).repeat(N),\n",
    "            'date' : np.tile(np.asarray(frame.index), K)}\n",
    "    \n",
    "    # Return the DataFrame\n",
    "    return DataFrame(data, columns=['date', 'variable', 'value'])\n",
    "\n",
    "#Set the DataFrame we'll be using\n",
    "dframe = unpivot(tm.makeTimeDataFrame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d33b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the \"stacked\" data, note how there are multiple variables and values for the dates\n",
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da8ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's pivot the data\n",
    "\n",
    "# First two value spassed are teh row and column indexes, then finally an optional fill value\n",
    "dframe_piv = dframe.pivot('date','variable','value')\n",
    "\n",
    "#Show\n",
    "dframe_piv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b8cf7",
   "metadata": {},
   "source": [
    "#### Lec 35 - Duplicates in DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac669cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets get a dataframe with duplicates\n",
    "\n",
    "dframe = DataFrame({'key1': ['A'] * 2 + ['B'] * 3,\n",
    "                  'key2': [2, 2, 2, 3, 3]})\n",
    "\n",
    "#Show\n",
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede738e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use duplicated to find duplicates\n",
    "dframe.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de0e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also drop duplicates like this:\n",
    "dframe.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7805e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can filter which duplicates to drop by a single column\n",
    "dframe.drop_duplicates(['key1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3bb133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show original\n",
    "dframe\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af492045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#By default the first value was taken for the duplicates, we can also take the last value instead\n",
    "dframe.drop_duplicates(['key1'],take_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1eaf1b",
   "metadata": {},
   "source": [
    "#### Lec 36 - Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48328cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a dframe to work with (Highest elevation cities in USA)\n",
    "dframe = DataFrame({'city':['Alma','Brian Head','Fox Park'],\n",
    "                    'altitude':[3158,3000,2762]})\n",
    "\n",
    "#Show\n",
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b89b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's say we wanted to add a column for the States, we can do that with a mapping.\n",
    "state_map={'Alma':'Colorado','Brian Head':'Utah','Fox Park':'Wyoming'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff681ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can map that data to our current dframe\n",
    "dframe['state'] = dframe['city'].map(state_map)\n",
    "     \n",
    "\n",
    "#Show result\n",
    "dframe\n",
    "\n",
    "# Mapping is a great way to do element-wise transfomations and other data cleaning operations!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc26e5ff",
   "metadata": {},
   "source": [
    "#### Lec 37 - Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make  Series\n",
    "ser1 = Series([1,2,3,4,1,2,3,4])\n",
    "#Show\n",
    "ser1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b0bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using replace we can select --> .replace(value to be replaced, new_value)\n",
    "ser1.replace(1,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dedb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can also input lists\n",
    "ser1.replace([1,4],[100,400])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0295d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can also input dictionary\n",
    "ser1.replace({4:np.nan})\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7007086f",
   "metadata": {},
   "source": [
    "#### Lec 38 - Rename Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adcda01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Making a DataFrame\n",
    "dframe= DataFrame(np.arange(12).reshape((3, 4)),\n",
    "                 index=['NY', 'LA', 'SF'],\n",
    "                 columns=['A', 'B', 'C', 'D'])\n",
    "\n",
    "#Show\n",
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db581ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like a Series, axis indexes can also use map\n",
    "\n",
    "#Let's use map to lowercase the city initials\n",
    "dframe.index.map(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebc3662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to assign this to the actual index, you can use index\n",
    "dframe.index = dframe.index.map(str.lower)\n",
    "#Show\n",
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296810b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use rename if you want to create a transformed version withour modifying the original!\n",
    "\n",
    "#str.title will capitalize the first letter, lowercasing the columns\n",
    "dframe.rename(index=str.title, columns=str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6ee9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use rename to insert dictionaries providing new values for indexes or columns!\n",
    "dframe.rename(index={'ny': 'NEW YORK'},\n",
    "            columns={'A': 'ALPHA'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df07a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you would like to actually edit the data set in place, set inplace=True\n",
    "dframe.rename(index={'ny': 'NEW YORK'}, inplace=True)\n",
    "dframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdbdaf2",
   "metadata": {},
   "source": [
    "#### Lec 39 - Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc72f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [1990,1991,1992,2008,2012,2015,1987,1969,2013,2008,1999]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9768da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can seperate these years by decade\n",
    "decade_bins = [1960,1970,1980,1990,2000,2010,2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f212ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we'll use cut to get somethign called a Category object\n",
    "decade_cat = pd.cut(years,decade_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80825ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show\n",
    "decade_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check the categories using .categories\n",
    "decade_cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a272af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we can check the value counts in each category\n",
    "pd.value_counts(decade_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae4f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also pass data values to the cut.\n",
    "\n",
    "#For instance, if we just wanted to make two bins, evenly spaced based on max and min year, with a 1 year precision\n",
    "pd.cut(years,2,precision=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e5f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thats about it for binning basics\n",
    "# One last thing to note, jus tlike in standard math notation, when setting up bins:\n",
    "# () means open, while [] means closed/inclusive\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f803a30a",
   "metadata": {},
   "source": [
    "#### Lec 40 - Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc5b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's see how we would find outliers in a dataset\n",
    "\n",
    "# First we'll seed the numpy generator\n",
    "np.random.seed(12345)\n",
    "\n",
    "#Next we'll create the dataframe\n",
    "dframe = DataFrame(np.random.randn(1000,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd09c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show preview\n",
    "dframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets describe the data\n",
    "dframe.describe()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f88dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lets select the first column\n",
    "col = dframe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2977e973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOw we can check which values in the column are greater than 3, for instance.\n",
    "col[np.abs(col)>3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc356ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we now know in column[0], rows 523 and 900 have values with abs > 3\n",
    "\n",
    "#How about all the columns?\n",
    "\n",
    "# We can use the \"any\" method\n",
    "dframe[(np.abs(dframe)>3).any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0950ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE could also possibly cap the data at 3\n",
    "\n",
    "dframe[np.abs(dframe)>3] = np.sign(dframe) *3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f44b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe.describe()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ab0e6c",
   "metadata": {},
   "source": [
    "#### Lec 41 - Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32645bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE can randomly reorder (permutate) a Series, or the rows in a DataFrame\n",
    "\n",
    "#Let's take a look\n",
    "dframe = DataFrame(np.arange(4 * 4).reshape((4, 4)))\n",
    "\n",
    "#Create an array with a random perumation of 0,1,2,3\n",
    "blender = np.random.permutation(4)\n",
    "\n",
    "blender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf4783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05559c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now permutate the dframe based on the blender\n",
    "dframe.take(blender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d921b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now what if we want permuations WITH replacement\n",
    "     \n",
    "\n",
    "# Let imagine a box with 3 marbles in it: labeled 1, 2, and 3\n",
    "box = np.array([1,2,3])\n",
    "\n",
    "# Now lets create a random permuation WITH replacement using randint\n",
    "shaker = np.random.randint(0, len(box), size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b890569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check teh box \"shaker\"\n",
    "shaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908fb36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets grab form the box\n",
    "hand_grabs = box.take(shaker)\n",
    "\n",
    "#show\n",
    "hand_grabs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd8d9a2",
   "metadata": {},
   "source": [
    "Congratulations! We're all done with this Section. Up next: Working with Data Part 3 !!!\n",
    "\n",
    "#### Lec 42 - GroupBy on DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d08f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make a dframe\n",
    "dframe = DataFrame({'k1':['X','X','Y','Y','Z'],\n",
    "                    'k2':['alpha','beta','alpha','beta','alpha'],\n",
    "                    'dataset1':np.random.randn(5),\n",
    "                    'dataset2':np.random.randn(5)})\n",
    "\n",
    "#Show\n",
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d20be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's see how to use groupby\n",
    "\n",
    "#Lets grab the dataset1 column and group it by the k1 key\n",
    "group1 = dframe['dataset1'].groupby(dframe['k1'])\n",
    "\n",
    "#Show the groupby object\n",
    "group1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can perform operations on this particular group\n",
    "group1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We can use group keys that are series as well\n",
    "\n",
    "#For example:\n",
    "\n",
    "#We'll make some arrays for use as keys\n",
    "cities = np.array(['NY','LA','LA','NY','NY'])\n",
    "month = np.array(['JAN','FEB','JAN','FEB','JAN'])\n",
    "\n",
    "#Now using the data from dataset1, group the means by city and month\n",
    "dframe['dataset1'].groupby([cities,month]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe5e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see the original dframe again.\n",
    "dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3afc28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# WE can also pass column names as group keys\n",
    "dframe.groupby('k1').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d5077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or multiple column names\n",
    "dframe.groupby(['k1','k2']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa398ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Another useful groupby method is getting the group sizes\n",
    "dframe.groupby(['k1']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15270d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also iterate over groups\n",
    "\n",
    "#For example:\n",
    "for name,group in dframe.groupby('k1'):\n",
    "    print \"This is the %s group\" %name\n",
    "    print group\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d1b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also iterate with multiple keys\n",
    "for (k1,k2) , group in dframe.groupby(['k1','k2']):\n",
    "    print \"Key1 = %s Key2 = %s\" %(k1,k2)\n",
    "    print group\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2203d164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A possibly useful tactic is creating a dictionary of the data pieces \n",
    "group_dict = dict(list(dframe.groupby('k1')))\n",
    "\n",
    "#Show the group with X\n",
    "group_dict['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2af76ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could have also chosen to do this with axis = 1\n",
    "\n",
    "# Let's creat a dictionary for dtypes of objects!\n",
    "group_dict_axis1 = dict(list(dframe.groupby(dframe.dtypes,axis=1)))\n",
    "\n",
    "#show\n",
    "group_dict_axis1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af74d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we'll learn how to use groupby with columns\n",
    "\n",
    "# For example if we only wanted to group the dataset2 column with both sets of keys\n",
    "dataset2_group = dframe.groupby(['k1','k2'])[['dataset2']]\n",
    "\n",
    "dataset2_group.mean()\n",
    "\n",
    "#Next we'll have a quick lesson on grouping with dictionaries and series!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec2ab50",
   "metadata": {},
   "source": [
    "#### Lec 43 - Groupby on Dict and Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a409a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's make a Dframe\n",
    "\n",
    "animals = DataFrame(np.arange(16).reshape(4, 4),\n",
    "                   columns=['W', 'X', 'Y', 'Z'],\n",
    "                   index=['Dog', 'Cat', 'Bird', 'Mouse'])\n",
    "\n",
    "#Now lets add some NAN values\n",
    "animals.ix[1:2, ['W', 'Y']] = np.nan \n",
    "\n",
    "#Show\n",
    "animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9fc853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's say I had a dictionary with ebhavior values in it\n",
    "behavior_map = {'W': 'good', 'X': 'bad', 'Y': 'good','Z': 'bad'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ac550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can groupby using that mapping\n",
    "animal_col = animals.groupby(behavior_map, axis=1)\n",
    "\n",
    "# Show the sum accroding to the groupby with the mapping\n",
    "animal_col.sum()\n",
    "\n",
    "# For example [dog][good] = [dog][Y]+[dog][W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76de0275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try it with a Series\n",
    "behav_series = Series(behavior_map)\n",
    "\n",
    "#Show\n",
    "behav_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d46635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's groupby the Series\n",
    "\n",
    "animals.groupby(behav_series, axis=1).count()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb93b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also groupby with functions!\n",
    "\n",
    "#Show our dframe again\n",
    "animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d977522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets assume we wanted to group by the length of the animal names, we can pass the len function into groupby!\n",
    "\n",
    "# Show\n",
    "animals.groupby(len).sum()\n",
    "\n",
    "#Note the index is now number of letters in the animal name\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf8559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also mix functions with arrays,dicts, and Series for groupby methods\n",
    "\n",
    "# Set a list for keys\n",
    "keys = ['A', 'B', 'A', 'B']\n",
    "\n",
    "# Now groupby length of name and the keys to show max values\n",
    "animals.groupby([len, keys]).max()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We can also use groupby with hierarchaly index levels\n",
    "\n",
    "#Create a hierarchal column index\n",
    "hier_col = pd.MultiIndex.from_arrays([['NY','NY','NY','SF','SF'],[1,2,3,1,2]],names=['City','sub_value'])\n",
    "\n",
    "# Create a dframe with hierarchal index\n",
    "dframe_hr = DataFrame(np.arange(25).reshape(5,5),columns=hier_col)\n",
    "\n",
    "#Multiply values by 100 for clarity\n",
    "dframe_hr = dframe_hr*100\n",
    "\n",
    "#Show\n",
    "dframe_hr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81409a8f",
   "metadata": {},
   "source": [
    "#### Lec 44 - Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a784584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Agrregation consists of operations that result in a scalar (e.g. mean(),sum(),count(), etc)\n",
    "\n",
    "#Let's get a csv data set to play with\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/'\n",
    "\n",
    "\n",
    "# Save thewinquality.csv file in the same folder as your ipython notebooks, note the delimiter used ;\n",
    "dframe_wine = pd.read_csv('winequality_red.csv',sep=';')\n",
    "     \n",
    "\n",
    "# Let's get a preview\n",
    "dframe_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651b6f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about we find out the average alcohol content for the wine\n",
    "dframe_wine['alcohol'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f41ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That was an example of an aggregate, how about we make our own?\n",
    "def max_to_min(arr):\n",
    "    return arr.max() - arr.min()\n",
    "\n",
    "# Let's group the wines by \"quality\"\n",
    "wino = dframe_wine.groupby('quality')\n",
    "\n",
    "# Show\n",
    "wino.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6543b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now apply our own aggregate function, this function takes the max value of the col and subtracts the min value of the col\n",
    "wino.agg(max_to_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bc6b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also pass string methods through aggregate\n",
    "wino.agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210115a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go back to the original dframe\n",
    "dframe_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's adda  quality to alcohol content ratio\n",
    "dframe_wine['qual/alc ratio'] = dframe_wine['quality']/dframe_wine['alcohol']\n",
    "     \n",
    "# Show\n",
    "dframe_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769d414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE can also use pivot tables instead of groupby\n",
    "\n",
    "# Pivot table of quality\n",
    "dframe_wine.pivot_table(index=['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5e7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "dframe_wine.plot(kind='scatter',x='quality',y='alcohol')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904d38aa",
   "metadata": {},
   "source": [
    "We can see that the data is probably better fit for a box plot for a more concise view of the data See if you can figure how to get a boxplot using the pandas documentation and what you have learned so far\n",
    "\n",
    "Don't worry if you can't quite figure it out just yet, the next section will cover all sorts of data visualizations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd111ec",
   "metadata": {},
   "source": [
    "#### Lec 45 - Splitting, Applying and Combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c76064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's grab the wine data again\n",
    "dframe_wine = pd.read_csv('winequality_red.csv',sep=';')\n",
    "\n",
    "#Preview\n",
    "dframe_wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a5cbaf",
   "metadata": {},
   "source": [
    "What if we wanted to know the highest alcohol content for each quality range?\n",
    "\n",
    "We can use groupby mechanics to split-apply-combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19080bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that assigns a rank to each wine based on alcohol content, with 1 being the highest alcohol content\n",
    "def ranker(df):\n",
    "    df['alc_content_rank'] = np.arange(len(df)) + 1\n",
    "    return df\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a66e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now sort the dframe by alcohol in ascending order\n",
    "dframe_wine.sort('alcohol',ascending=False,inplace=True)\n",
    "\n",
    "# Now we'll group by quality and apply our ranking function\n",
    "dframe_wine = dframe_wine.groupby('quality').apply(ranker)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf27ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preview\n",
    "dframe_wine.head()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c3464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now finally we can just call for the dframe where the alc_content_rank == 1\n",
    "\n",
    "# Get the numebr of quality counts\n",
    "num_of_qual = dframe_wine['quality'].value_counts()\n",
    "\n",
    "#Show\n",
    "num_of_qual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dcc4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll show the combined info for teh wines that had the highest alcohol content for their respective rank!\n",
    "dframe_wine[dframe_wine.alc_content_rank == 1].head(len(num_of_qual))\n",
    " \n",
    "# Awesome! Ask yourself if there are any trends you would like to find in this data?\n",
    "# Is there a relationship between wine ranking and alcohol content?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c410ba6",
   "metadata": {},
   "source": [
    "#### Lec 46 - Cross-Tabulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae891903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a quick data set\n",
    "from StringIO import StringIO\n",
    "\n",
    "data =\"\"\"\\\n",
    "Sample   Animal   Intelligence\n",
    "1        Dog     Smart\n",
    "2 Dog Smart\n",
    "3 Cat Dumb\n",
    "4 Cat Dumb\n",
    "5 Dog Dumb\n",
    "6 Cat Smart\"\"\"\n",
    "\n",
    "#Store as dframe\n",
    "dframe = pd.read_table(StringIO(data),sep='\\s+')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb257db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show\n",
    "dframe\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee850b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create a cross-tabulation table, which is basically just a frequency table\n",
    "pd.crosstab(dframe.Animal,dframe.Intelligence,margins=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d40456",
   "metadata": {},
   "source": [
    "#### Lec 47 - Installing Seaborn\n",
    "\n",
    "To install file the directions at the following link, you should be able to use a simple pip install. Remember to install the dependencies!\n",
    "\n",
    "http://stanford.edu/~mwaskom/software/seaborn/installing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe549c9f",
   "metadata": {},
   "source": [
    "#### Lec 48 - Histograms\n",
    "\n",
    "First of all, source of information for what a histogram actually is: http://en.wikipedia.org/wiki/Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801844a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The normal imports\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "import pandas as pd\n",
    "\n",
    "# Import the stats librayr from numpy\n",
    "from scipy import stats\n",
    "\n",
    "# These are the plotting modules adn libraries we'll use:\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Command so that plots appear in the iPython Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64028bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a random normal-dist dataset\n",
    "dataset1 = randn(100)\n",
    "\n",
    "#Plot a histogram of the dataset, note bins=10 by default\n",
    "plt.hist(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00ab8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make another dataset\n",
    "dataset2 = randn(80)\n",
    "\n",
    "#Plot\n",
    "plt.hist(dataset2,color='indianred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fca848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use normed to plot on same plot\n",
    "\n",
    "# Set normed=True for the plots to be normalized in order to comapre data sets with different number of observations\n",
    "# Set alpha=0.5 for transperancy\n",
    "\n",
    "plt.hist(dataset1,normed=True,color='indianred',alpha=0.5,bins=20)\n",
    "plt.hist(dataset2,normed=True,alpha=0.5,bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c9b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make two more random normal dist data sets\n",
    "data1 = randn(1000)\n",
    "data2 = randn(1000)\n",
    "\n",
    "#Can represent joint distributions using joint plots\n",
    "sns.jointplot(data1,data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405295e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also use hex bins for a more concise picture\n",
    "sns.jointplot(data1,data2,kind='hex')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8899c5a2",
   "metadata": {},
   "source": [
    "#### Lec 49 - Kernel Density Estimation Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed10062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start off with a carpet/rug plot\n",
    "# A rug plot simpot puts ticks wherever a value occured\n",
    "\n",
    "#Create dataset\n",
    "dataset = randn(25)\n",
    "#Create rugplot\n",
    "sns.rugplot(dataset)\n",
    "#Set y-axis limit\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18755136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram on top of \n",
    "plt.hist(dataset,alpha=0.3)\n",
    "sns.rugplot(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1266fda",
   "metadata": {},
   "source": [
    "The histogram sets up 10 bins and then just count how many ticks appeared in each bin, setting the height of each bar\n",
    "\n",
    "The kernel density plot will represent each tick mark with a gaussian basis function. Let's see how we would do this manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce1c16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another rugplot\n",
    "sns.rugplot(dataset);\n",
    "\n",
    "# Set up the x-axis for the plot\n",
    "x_min = dataset.min() - 2\n",
    "x_max = dataset.max() + 2\n",
    "\n",
    "# 100 equally spaced points from x_min to x_max\n",
    "x_axis = np.linspace(x_min,x_max,100)\n",
    "\n",
    "# Set up the bandwidth, for info on this:\n",
    "url = 'http://en.wikipedia.org/wiki/Kernel_density_estimation#Practical_estimation_of_the_bandwidth'\n",
    "\n",
    "bandwidth = ((4*dataset.std()**5)/(3*len(dataset)))**.2\n",
    "\n",
    "\n",
    "# Create an empty kernel list\n",
    "kernel_list = []\n",
    "\n",
    "# Plot each basis function\n",
    "for data_point in dataset:\n",
    "    \n",
    "    # Create a kernel for each point and append to list\n",
    "    kernel = stats.norm(data_point,bandwidth).pdf(x_axis)\n",
    "    kernel_list.append(kernel)\n",
    "    \n",
    "    #Scale for plotting\n",
    "    kernel = kernel / kernel.max()\n",
    "    kernel = kernel * .4\n",
    "    plt.plot(x_axis,kernel,color = 'grey',alpha=0.5)\n",
    "\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b7e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the kde plot we can sum these basis functions.\n",
    "\n",
    "\n",
    "\n",
    "# Plot the sum of the basis function\n",
    "sum_of_kde = np.sum(kernels,axis=0)\n",
    "\n",
    "# Plot figure\n",
    "fig = plt.plot(x_axis,sum_of_kde,color='indianred')\n",
    "\n",
    "# Add the initial rugplot\n",
    "sns.rugplot(dataset,c = 'indianred')\n",
    "\n",
    "# Get rid of y-tick marks\n",
    "plt.yticks([])\n",
    "\n",
    "# Set title\n",
    "plt.suptitle(\"Sum of the Basis Functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b5c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can see how to do it in one step with seaborn! Awesome!\n",
    "sns.kdeplot(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e65235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can adjust the bandwidth of the sns kde to make the kde plot more or less sensitive to high frequency\n",
    "\n",
    "# Rugplot\n",
    "sns.rugplot(dataset,color='black')\n",
    "\n",
    "# Plot various bandwidths\n",
    "for bw in np.arange(0.5,2,0.25):\n",
    "    sns.kdeplot(dataset,bw=bw,lw=1.8,label=bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b10dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also choose different kernels\n",
    "\n",
    "kernel_options = [\"biw\", \"cos\", \"epa\", \"gau\", \"tri\", \"triw\"]\n",
    "\n",
    "# More info on types\n",
    "url = 'http://en.wikipedia.org/wiki/Kernel_(statistics)'\n",
    "\n",
    "# Use label to set legend\n",
    "for kern in kernel_options:\n",
    "    sns.kdeplot(dataset,kernel=kern,label=kern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0aec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also shade if desired\n",
    "for kern in kernel_options:\n",
    "    sns.kdeplot(dataset,kernel=kern,label=kern,shade=True,alpha=0.5)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For vertical axis, use the vertical keyword\n",
    "sns.kdeplot(dataset,vertical=True)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc8972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we can also use kde plot to create a cumulative distribution function (CDF) of the data\n",
    "\n",
    "# URL for info on CDF\n",
    "url = 'http://en.wikipedia.org/wiki/Cumulative_distribution_function'\n",
    "\n",
    "sns.kdeplot(dataset,cumulative=True)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9e3304",
   "metadata": {},
   "source": [
    "### Multivariate Density Estimation using kdeplot\n",
    "\n",
    "We can also use kdeplot for multidimensional data. Lets see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a new dataset\n",
    "\n",
    "# Mean center of data\n",
    "mean = [0,0]\n",
    "\n",
    "# Diagonal covariance\n",
    "cov = [[1,0],[0,100]]\n",
    "\n",
    "# Create dataset using numpy\n",
    "dataset2 = np.random.multivariate_normal(mean,cov,1000)\n",
    "\n",
    "# Bring back our old friend pandas\n",
    "dframe = pd.DataFrame(dataset2,columns=['X','Y'])\n",
    "\n",
    "# Plot our dataframe\n",
    "sns.kdeplot(dframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d449c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could have also passed two vectors seperately, and shade\n",
    "sns.kdeplot(dframe.X,dframe.Y,shade=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ebb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can specify a particualr bandwidth\n",
    "ns.kdeplot(dframe,bw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c48cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or just use silverman again\n",
    "sns.kdeplot(dframe,bw='silverman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ba99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also create a kde joint plot, simliar to the hexbin plots we saw before\n",
    "\n",
    "sns.jointplot('X','Y',dframe,kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd5b96c",
   "metadata": {},
   "source": [
    "#### Lec 50 - Combining Plot Styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164698f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'l learn how to combine plot styles\n",
    "     \n",
    "\n",
    "# Create datset\n",
    "dataset = randn(100)\n",
    "\n",
    "# Use distplot for combining plots, by default a kde over a histogram is shown\n",
    "sns.distplot(dataset,bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4511427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist, rug, and kde are all input arguments to turn those plots on or off\n",
    "sns.distplot(dataset,rug=True,hist=False)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5774c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO control specific plots in distplot, use [plot]_kws argument with dictionaries.\n",
    "\n",
    "#Here's an example\n",
    "\n",
    "sns.distplot(dataset,bins=25,\n",
    "             kde_kws={'color':'indianred','label':'KDE PLOT'},\n",
    "             hist_kws={'color':'blue','label':\"HISTOGRAM\"})\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f071a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE can also use pandas data objects for this\n",
    "\n",
    "from pandas import Series\n",
    "\n",
    "# Create Series form dataset\n",
    "ser1 = Series(dataset,name='My_DATA')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1110e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Series\n",
    "sns.distplot(ser1,bins=25)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622a4b5",
   "metadata": {},
   "source": [
    "#### Lec 51 - Box and Violin Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d250ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll learn about box and violin plots\n",
    "url = 'http://en.wikipedia.org/wiki/Box_plot#mediaviewer/File:Boxplot_vs_PDF.svg'\n",
    "\n",
    "# Let's create two distributions\n",
    "data1 = randn(100)\n",
    "data2 = randn(100) + 2 # Off set the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff3916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create a box plot\n",
    "sns.boxplot([data1,data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853de042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how the previous plot had outlier points, we can include those with the \"whiskers\"\n",
    "sns.boxplot([data1,data2],whis=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE can also set horizontal by setting vertical to false\n",
    "sns.boxplot([data1,data2],whis=np.inf, vert = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7475617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While box plots are great, they can sometimes not give the full picture\n",
    "\n",
    "# Violin/Viola plots can combine the simplicity of a box plot with the information of a kde plot\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cdb143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create an example where a box plot doesn't give the whole picture\n",
    "\n",
    "# Normal Distribution\n",
    "data1 = stats.norm(0,5).rvs(100)\n",
    "\n",
    "# Two gamma distributions concatenated together (Second one is inverted)\n",
    "data2 = np.concatenate([stats.gamma(5).rvs(50)-1,\n",
    "                        -1*stats.gamma(5).rvs(50)])\n",
    "\n",
    "# Box plot them\n",
    "sns.boxplot([data1,data2],whis=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877b61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the above plots, you may think that the distributions are fairly similar\n",
    "# But lets check out what a violin plot reveals\n",
    "sns.violinplot([data1,data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc303759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wow, quite revealing!\n",
    "     \n",
    "\n",
    "# We can also change the bandwidth of the kernel used for the density fit of the violin plots if desired\n",
    "sns.violinplot(data2,bw=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29c7d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Much like a rug plot, we can also include the individual points, or sticks\n",
    "sns.violinplot(data1,inner=\"stick\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2c5f83",
   "metadata": {},
   "source": [
    "#### Lec 52 - Regression Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll learn how ot visualize multiple regression with lmplot()\n",
    "\n",
    "# Luckily, Seaborn comes with an example dataset to use as a pandas DataFrame\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "     \n",
    "\n",
    "# Preview\n",
    "tips.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef59aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use lmplot() to plot the total bill versus tips\n",
    "sns.lmplot(\"total_bill\",\"tip\",tips)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a24e37",
   "metadata": {},
   "source": [
    " First we can see a scatter plot of all the points, tip vs total_bill\n",
    " \n",
    " Then we see a linear regression line, which is an estimateed linear fit model to the data\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d608f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE can also specify teh confidence interval to use for the linear fit\n",
    "\n",
    "sns.lmplot(\"total_bill\",\"tip\",tips,ci=75) # 68% ci \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241b9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like before, we can use dictionaries to edit individual parts of the plot\n",
    "\n",
    "sns.lmplot(\"total_bill\", \"tip\", tips,\n",
    "           scatter_kws={\"marker\": \"o\", \"color\": \"indianred\"},\n",
    "           line_kws={\"linewidth\": 1, \"color\": \"blue\"});\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f8543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE can also check out higher-order trends\n",
    "sns.lmplot(\"total_bill\", \"tip\", tips,order=4,\n",
    "           scatter_kws={\"marker\": \"o\", \"color\": \"indianred\"},\n",
    "           line_kws={\"linewidth\": 1, \"color\": \"blue\"})\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3045b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also not fit a regression if desired\n",
    "sns.lmplot(\"total_bill\", \"tip\", tips,fit_reg=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4bbe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmplot() also works on discrete variables, such as the percentage of the tip\n",
    "\n",
    "# Create a new column for tip percentage\n",
    "tips[\"tip_pect\"]=100*(tips['tip']/tips['total_bill'])\n",
    "\n",
    "#plot\n",
    "sns.lmplot(\"size\", \"tip_pect\", tips);\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d470beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also add jitter to this\n",
    "\n",
    "#Info link\n",
    "url = \"http://en.wikipedia.org/wiki/Jitter\"\n",
    "\n",
    "#plot\n",
    "sns.lmplot(\"size\", \"tip_pect\", tips,x_jitter=.1);\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75515bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also estimate the tendency of each bin (size of party in this case)\n",
    "sns.lmplot(\"size\", \"tip_pect\", tips, x_estimator=np.mean);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b7adc",
   "metadata": {},
   "source": [
    "Interesting, looks like there is more variance for party sizes of 1 then 2-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f89bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the hue facet to automatically define subsets along a column\n",
    "\n",
    "# Plot, note the markers argument\n",
    "sns.lmplot(\"total_bill\", \"tip_pect\", tips, hue=\"sex\",markers=[\"x\",\"o\"])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does day make a difference?\n",
    "sns.lmplot(\"total_bill\", \"tip_pect\", tips, hue=\"day\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11452bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally it should be noted that Seabron supports LOESS model fitting\n",
    "url = 'http://en.wikipedia.org/wiki/Local_regression'\n",
    "\n",
    "sns.lmplot(\"total_bill\", \"tip_pect\", tips, lowess=True, line_kws={\"color\": 'black'});\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b387690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lmplot() we've been using is actually using a lower-level function, regplot()\n",
    "\n",
    "sns.regplot(\"total_bill\",\"tip_pect\",tips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4eddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg_plot can be added to existing axes without modifying anything in the figure\n",
    "\n",
    "# Create figure with 2 subplots\n",
    "fig, (axis1,axis2) = plt.subplots(1,2,sharey =True)\n",
    "\n",
    "sns.regplot(\"total_bill\",\"tip_pect\",tips,ax=axis1)\n",
    "sns.violinplot(tips['tip_pect'],tips['size'],color='Reds_r',ax=axis2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b824c653",
   "metadata": {},
   "source": [
    "#### Lec 53 - Heatmaps and Clustered Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf68ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again seaborn comes with a great dataset to play and learn with\n",
    "flight_dframe = sns.load_dataset('flights')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preview\n",
    "flight_dframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef3b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pivot this dataframe do its easier to manage\n",
    "flight_dframe = flight_dframe.pivot(\"month\",\"year\",\"passengers\")\n",
    "\n",
    "#Show\n",
    "flight_dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3b1074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset is now in a clear format to be dispalyed as a heatmap\n",
    "sns.heatmap(flight_dframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f20017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also have the option to annotate each cell\n",
    "sns.heatmap(flight_dframe,annot=True,fmt='d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1947793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn will automatically try to pick the best color scheme for your dataset, whether is be diverging or converging colormap\n",
    "     \n",
    "\n",
    "# We can choose our own 'center' for our colormap\n",
    "sns.heatmap(flight_dframe,center=flight_dframe.loc['January',1955])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5800d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap() can be used on an axes for a subplot to create more informative figures\n",
    "f, (axis1,axis2) = plt.subplots(2,1)\n",
    "\n",
    "yearly_flights = flight_dframe.sum()\n",
    "\n",
    "# Since yearly_flights is a weird format, we'll have to grab the values we want with a Series, then put them in a dframe\n",
    "\n",
    "years = pd.Series(yearly_flights.index.values)\n",
    "years = pd.DataFrame(years)\n",
    "\n",
    "flights = pd.Series(yearly_flights.values) \n",
    "flights = pd.DataFrame(flights)\n",
    "\n",
    "# Make the dframe and name columns\n",
    "year_dframe = pd.concat((years,flights),axis=1)\n",
    "year_dframe.columns = ['Year','Flights']\n",
    "\n",
    "\n",
    "\n",
    "# Create the bar plot on top\n",
    "sns.barplot('Year',y='Flights',data=year_dframe, ax = axis1)\n",
    "\n",
    "# Create the heatmap on bottom\n",
    "sns.heatmap(flight_dframe,cmap='Blues',ax=axis2,cbar_kws={\"orientation\": \"horizontal\"})\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283df69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we'll learn about using a clustermap\n",
    "\n",
    "# Clustermap will reformat the heatmap so similar rows are next to each other\n",
    "sns.clustermap(flight_dframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956b62ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's uncluster the columns\n",
    "sns.clustermap(flight_dframe,col_cluster=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5173af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the number of flights increase every year, we should set a standard scale\n",
    "sns.clustermap(flight_dframe,standard_scale=1) # standardize by columns (year)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57767d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or scale the rows\n",
    "sns.clustermap(flight_dframe,standard_scale=0)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173604ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we can also normalize the rows by their Z-score.\n",
    "# This subtracts the mean and devides by the STD of each column, then teh rows have amean of 0 and a variance of 1\n",
    "sns.clustermap(flight_dframe,z_score=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4ce617",
   "metadata": {},
   "source": [
    "Above we can see which values are greater than the mean and which are below very clearly\n",
    "     \n",
    "\n",
    "CONGRATULATIONS!! We've developed quite a toolbox to hammer out some great data anaysis projects!\n",
    "\n",
    "Up next: Projects to apply what we've learned to real datasets!\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ae3857",
   "metadata": {},
   "source": [
    "### Introduction to SQL with Python\n",
    "\n",
    "In this notebook we'll go over a brief introduction to the structure of the Sakila Database and setting up SQL in your Python Environment.\n",
    "\n",
    "DISCLAIMER:\n",
    "\n",
    "There are many ways to browse through a SQL database, throughout this Appendix we are only going to be focusing on learning about SQL queries using a combination of SQLite,Python,pandas, and SQLAlchemy. Please note that this is a pretty specific way of operating with a SQL Database, and may or may not fit other general needs. The primary goal of this section is to teach you how to use SQL queries to grab information and set it as a pandas DataFrame. We will not be going over more general topics of relational databases, MySQL, or using a SQL console directly.\n",
    "\n",
    "To fully understand the content of this Appendix, I suggest you complete the course up to at least Lecture 28, although I really recommend completing up to Lecture 46 to get the most out of this Appendix!\n",
    "\n",
    "Great, let's begin!\n",
    "Step 1: Download SQL Alchemy\n",
    "To start this appendix, download SQLAlchemy. You can do this by either downloading it here\n",
    "\n",
    "Or - by typing pip install sqlalchemy in your command line.\n",
    "\n",
    "Or - by typing conda install sqlalchemy if you are using the Anaconda installation of Python. (recommended)\n",
    "\n",
    "Step 2: Download SQLite Broswer\n",
    "Next up we will download a sql browser. We will be using SQLite Browser because it is lightweight and free to use. There are many alternatives you can use, check out a list of 10 free ones here\n",
    "\n",
    "Download SQLite Browser here: http://sqlitebrowser.org/\n",
    "\n",
    "Step 3: Download the sakila Database\n",
    "You can download the fully constructed database here\n",
    "\n",
    "Or - you can download the .sql file to construct the database yourself: http://dev.mysql.com/doc/index-other.html Then use SQLite Browser to construct the database by running the .sql\n",
    "\n",
    "Either way, make sure to save it in the same directory as your iPython notebooks, or remember the file path for later so we can tell pandas exactly where to look for it.\n",
    "\n",
    "All done! Now let's look at the database before diving into how to work with it in Python.\n",
    "Check out the database either by opening it up using SQLite Browser or by checking out the diagram at this link: Diagram\n",
    "\n",
    "I've posted it below as well inside this notebook, but fair warning, the picture is huge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb08493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The picture is really big, I suggest you check out the link directly!\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(url='http://www.dbquanti.eu/css/images/database.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd68eb8",
   "metadata": {},
   "source": [
    "Now that we have seen an overview of what the database looks like, let's go ahead and learn how to communicate with it with Python and pandas.\n",
    "\n",
    "Python comes with SQLite3, which provides a lightweight disk-based database that doesn't require a seperate server process. It's useful to prototyp with SQLite and then port the code to a larger database system, like MySQL. Python comes with a pretty awesome module to connect to a SQL database with SQLite. The module is SQLite3, let's go ahead and import it (and pandas as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d0f91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports!\n",
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7a809",
   "metadata": {},
   "source": [
    "To use the module, you must first create a Connection object that represents the database. If the database name already exists SQLite3 will automatically connect to it, if it does not exsist, SQLite3 will automatically create.\n",
    "\n",
    "For experienced users: You can also supply the special name :memory: to create a database in RAM.\n",
    "\n",
    "Let's make the connection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a78d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database (again, downloaded from here: https://www.dropbox.com/s/t049qmjzycrakro/sakila.db?dl=0\n",
    "con = sqlite3.connect(\"sakila.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a400d276",
   "metadata": {},
   "source": [
    "Now we can run a basic SQL query, pass it with pandas, and display the output as a DataFrame! Don't worry if you don't understand the query completely yet, this is just a usage example for connecting to the database, other lectures will dive deeper into SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1584cde2",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql ' SELECT * FROM customer ': no such table: customer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Anaconda/anaconda3/lib/python3.11/site-packages/pandas/io/sql.py:2202\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2201\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2202\u001b[0m     cur\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   2203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: customer",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m sql_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\u001b[38;5;124m SELECT * FROM customer \u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Use pandas to pass sql query using connection form SQLite3\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(sql_query, con)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Show the resulting DataFrame\u001b[39;00m\n\u001b[1;32m      8\u001b[0m df\n",
      "File \u001b[0;32m~/Desktop/Anaconda/anaconda3/lib/python3.11/site-packages/pandas/io/sql.py:635\u001b[0m, in \u001b[0;36mread_sql\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[0;32m--> 635\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_query(\n\u001b[1;32m    636\u001b[0m             sql,\n\u001b[1;32m    637\u001b[0m             index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[1;32m    638\u001b[0m             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    639\u001b[0m             coerce_float\u001b[38;5;241m=\u001b[39mcoerce_float,\n\u001b[1;32m    640\u001b[0m             parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[1;32m    641\u001b[0m             chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[1;32m    642\u001b[0m             dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    643\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    644\u001b[0m         )\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    647\u001b[0m         _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
      "File \u001b[0;32m~/Desktop/Anaconda/anaconda3/lib/python3.11/site-packages/pandas/io/sql.py:2266\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[1;32m   2255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[1;32m   2256\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2257\u001b[0m     sql,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2264\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2265\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[0;32m-> 2266\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(sql, params)\n\u001b[1;32m   2267\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[1;32m   2269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Anaconda/anaconda3/lib/python3.11/site-packages/pandas/io/sql.py:2214\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, sql, params)\u001b[0m\n\u001b[1;32m   2211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[1;32m   2213\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2214\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql ' SELECT * FROM customer ': no such table: customer"
     ]
    }
   ],
   "source": [
    "# Set SQL query as a comment\n",
    "sql_query = ''' SELECT * FROM customer '''\n",
    "\n",
    "# Use pandas to pass sql query using connection form SQLite3\n",
    "df = pd.read_sql(sql_query, con)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f0c6d",
   "metadata": {},
   "source": [
    "Congratulations! You just passed a SQL Query using pandas and Python! You're amazing! Subsequent lectures will go further into how to query with SQL, but if you already know SQL, you're good to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3539aa10",
   "metadata": {},
   "source": [
    "#### SQL SELECT,DISTINCT,WHERE,AND & OR\n",
    "\n",
    "SQL SELECT Statement\n",
    "\n",
    "The SELECT statement is used to select data from a database. The result is then stored in a result table, sometimes called the result-set.\n",
    "\n",
    "Syntax for SQL SELECT\n",
    "SELECT column_name FROM table_name\n",
    "\n",
    "We could also select multiple columns:\n",
    "\n",
    "SELECT column_name1,column_name2\n",
    "FROM table_name\n",
    "\n",
    "Or we could select everything in a table using *\n",
    "\n",
    "SELECT * FROM table_name\n",
    "\n",
    "To see how this and multiple other queries work, we'll connect to the database and make a function that automatically takes in our query and returns a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d3d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "con = sqlite3.connect(\"sakila.db\")\n",
    "\n",
    "# Set function as our sql_to_pandas\n",
    "\n",
    "def sql_to_df(sql_query):\n",
    "\n",
    "    # Use pandas to pass sql query using connection form SQLite3\n",
    "    df = pd.read_sql(sql_query, con)\n",
    "\n",
    "    # Show the resulting DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92cc23a",
   "metadata": {},
   "source": [
    "#### Selecting Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12508c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select multiple columns example\n",
    "query = ''' SELECT first_name,last_name\n",
    "            FROM customer; '''\n",
    "\n",
    "# Grab from first two columns\n",
    "sql_to_df(query).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fb9c64",
   "metadata": {},
   "source": [
    "#### Selecting Everything from table with *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aaa133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select multiple columns example\n",
    "query = ''' SELECT *\n",
    "            FROM customer; '''\n",
    "\n",
    "# Grab \n",
    "sql_to_df(query).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa2db7a",
   "metadata": {},
   "source": [
    "#### Syntax for the SQL DISTINCT Statement\n",
    "\n",
    "In a table, a column may contain duplicate values; and sometimes you only want to list the distinct (unique) values. The DISTINCT keyword can be used to return only distinct (unique) values.\n",
    "\n",
    "SELECT DISTINCT column_name\n",
    "\n",
    "FROM table_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b515afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select distinct country_ids from the city table.\n",
    "query = ''' SELECT DISTINCT(country_id)\n",
    "            FROM city'''\n",
    "\n",
    "sql_to_df(query).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b401b85",
   "metadata": {},
   "source": [
    "#### Syntax for the SQL WHERE\n",
    "The WHERE clause is used to filter records, the WHERE clause is used to extract only the records that fulfill the specific parameter.\n",
    "\n",
    "SELECT column_name\n",
    "\n",
    "FROM table_name\n",
    "\n",
    "WHERE column_name ( math operator) desired_value;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918faca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all customer info from the 1st store.\n",
    "query = ''' SELECT *\n",
    "            FROM customer\n",
    "            WHERE store_id = 1'''\n",
    "\n",
    "sql_to_df(query).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a927a",
   "metadata": {},
   "source": [
    "Note, there are a variety of logical operators you can use for a SQL request.\n",
    "\n",
    "Operator\tDescription\n",
    "    %        Equal\n",
    "    \n",
    "<>\t        Not equal. Note: In some versions of SQL this operator             may be written !=\n",
    "\n",
    ">\t        Greater than\n",
    "\n",
    "<\t        Less than\n",
    "\n",
    ">=\t        Greater than or equal\n",
    "\n",
    "<=\t        Less than or equal\n",
    "\n",
    "SQL requires single quotes around text values, while numeric fields are not enclosed in quotes, for example a text value for the above where statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8239f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all customer info from Mary.\n",
    "query = ''' SELECT *\n",
    "            FROM customer\n",
    "            WHERE first_name = 'MARY'  '''\n",
    "\n",
    "sql_to_df(query).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0512ec1",
   "metadata": {},
   "source": [
    "#### Syntax for AND\n",
    "The AND operator is used to filter records based on more than one condition.\n",
    "\n",
    "The AND operator displays a record if both the first condition AND the second condition are true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef2c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all films from 2006 that are rated R.\n",
    "\n",
    "query = ''' SELECT *\n",
    "            FROM film\n",
    "            WHERE release_year = 2006\n",
    "            AND rating = 'R' '''\n",
    "\n",
    "sql_to_df(query).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1d80ed",
   "metadata": {},
   "source": [
    "#### Syntax for OR\n",
    "The OR operator displays a record if either the first condition OR the second condition is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c911a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select all films from R or PG.\n",
    "\n",
    "query = ''' SELECT *\n",
    "            FROM film\n",
    "            WHERE rating = 'PG'\n",
    "            OR rating = 'R' '''\n",
    "\n",
    "sql_to_df(query).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e73ffa",
   "metadata": {},
   "source": [
    "#### SQL WILDCARDS, ORDER BY, GROUP BY and Aggregate Functions\n",
    "In this section, we will go over Wildcard statements, as well as ORDER BY and GROUP BY statements.\n",
    "\n",
    "We will start by importing and connceting to our SQL database, then creating the function to convert SQL queries to a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b50e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "con = sqlite3.connect(\"sakila.db\")\n",
    "\n",
    "# Set function as our sql_to_pandas\n",
    "\n",
    "def sql_to_df(sql_query):\n",
    "\n",
    "    # Use pandas to pass sql query using connection form SQLite3\n",
    "    df = pd.read_sql(sql_query, con)\n",
    "\n",
    "    # Show the resulting DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa74db6",
   "metadata": {},
   "source": [
    "Before we begin with Wildcards, ORDER BY, and GROUP BY. Let's take a look at aggregate functions.\n",
    "\n",
    "- AVG() - Returns the average value.\n",
    "- COUNT() - Returns the number of rows.\n",
    "- FIRST() - Returns the first value.\n",
    "- LAST() - Returns the last value.\n",
    "- MAX() - Returns the largest value.\n",
    "- MIN() - Returns the smallest value.\n",
    "- SUM() - Returns the sum.\n",
    "\n",
    "You can call any of these aggregate functions on a column to get the resulting values back. For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37cdf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of customers\n",
    "query = ''' SELECT COUNT(customer_id)\n",
    "            FROM customer; '''\n",
    "\n",
    "# Grab \n",
    "sql_to_df(query).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3e5dfe",
   "metadata": {},
   "source": [
    "Go ahead and experiment with the other aggregate functions. The usual syntax is:\n",
    "\n",
    "SELECT column_name, aggregate_function(column_name)\n",
    "FROM table_name\n",
    "WHERE column_name\n",
    "\n",
    "SQL Wildcards\n",
    "A wildcard character can be used to substitute for any other characters in a string. In SQL, wildcard characters are used with the SQL LIKE operator. The LIKE operator is used in a WHERE clause to search for a specified pattern in a column.\n",
    "\n",
    "There are several wildcard operators:\n",
    "\n",
    "Wildcard\tDescription\n",
    "\n",
    "%\tA substitute for zero or more characters\n",
    "\n",
    "_\tA substitute for a single character\n",
    "\n",
    "[character_list]\tSets and ranges of characters to match\n",
    "Let's see them in action now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181e1603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First the % wildcard\n",
    "\n",
    "# Select any customers whose name start with an M\n",
    "query = ''' SELECT *\n",
    "            FROM customer\n",
    "            WHERE first_name LIKE 'M%' ; '''\n",
    "\n",
    "# Grab \n",
    "sql_to_df(query).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d7238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next the _ wildcard\n",
    "\n",
    "# Select any customers whose last name ends with ing\n",
    "query = ''' SELECT *\n",
    "            FROM customer\n",
    "            WHERE last_name LIKE '_ING' ; '''\n",
    "\n",
    "# Grab \n",
    "sql_to_df(query).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5bbce8",
   "metadata": {},
   "source": [
    "Now we will move on to the [Character_list] wildcard.\n",
    "\n",
    "**IMPORTANT NOTE!**\n",
    "\n",
    "Using [charlist] with SQLite is a little different than with other SQL formats, such as MySQL.\n",
    "\n",
    "In MySQL you would use:\n",
    "\n",
    "WHERE value LIKE '[charlist]%'\n",
    "\n",
    "In SQLite you use:\n",
    "\n",
    "WHERE value GLOB '[charlist]*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce8185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally the [character_list] wildcard\n",
    "\n",
    "# Select any customers whose first name begins with an A or a B\n",
    "query = ''' SELECT *\n",
    "            FROM customer\n",
    "            WHERE first_name GLOB '[AB]*' ; '''\n",
    "\n",
    "# Grab \n",
    "sql_to_df(query).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd47d7f",
   "metadata": {},
   "source": [
    "#### SQL ORDER BY\n",
    "The ORDER BY keyword is used to sort the result-set by one or more columns. The ORDER BY keyword sorts the records in ascending order by default. To sort the records in a descending order, you can use the DESC keyword. The syntax is:\n",
    "\n",
    "SELECT column_name\n",
    "FROM table_name\n",
    "ORDER BY column_name ASC|DESC\n",
    "\n",
    "Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all customers and order results by last name\n",
    "query = ''' SELECT *\n",
    "            FROM customer\n",
    "            ORDER BY last_name ; '''\n",
    "\n",
    "# Grab \n",
    "sql_to_df(query).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c71945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all customers and order results by last name, DESCENDING\n",
    "query = ''' SELECT *\n",
    "            FROM customer\n",
    "            ORDER BY last_name DESC; '''\n",
    "\n",
    "# Grab \n",
    "sql_to_df(query).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43836427",
   "metadata": {},
   "source": [
    "#### SQL GROUP BY\n",
    "The GROUP BY statement is used with the aggregate functions to group the results by one or more columns. The syntax is:\n",
    "\n",
    "SELECT column_name, aggregate_function(column_name)\n",
    "FROM table_name\n",
    "WHERE column_name operator value\n",
    "GROUP BY column_name;\n",
    "\n",
    "Let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b2026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of customers per store\n",
    "\n",
    "query = ''' SELECT store_id , COUNT(customer_id)\n",
    "            FROM customer\n",
    "            GROUP BY store_id; '''\n",
    "\n",
    "# Grab \n",
    "sql_to_df(query).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aad558",
   "metadata": {},
   "source": [
    "#### Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f999ce7d",
   "metadata": {},
   "source": [
    "**Web Scraping in Python**\n",
    "\n",
    "In this appendix lecture we'll go over how to scrape information from the web using Python.\n",
    "\n",
    "We'll go to a website, decide what information we want, see where and how it is stored, then scrape it and set it as a pandas DataFrame!\n",
    "Some things you should consider before web scraping a website:\n",
    "1.) You should check a site's terms and conditions before you scrape them.\n",
    "\n",
    "2.) Space out your requests so you don't overload the site's server, doing this could get you blocked.\n",
    "\n",
    "3.) Scrapers break after time - web pages change their layout all the time, you'll more than likely have to rewrite your code.\n",
    "\n",
    "4.) Web pages are usually inconsistent, more than likely you'll have to clean up the data after scraping it.\n",
    "\n",
    "5.) Every web page and situation is different, you'll have to spend time configuring your scraper.\n",
    "\n",
    "To learn more about HTML I suggest theses two resources:\n",
    "W3School\n",
    "\n",
    "Codecademy\n",
    "\n",
    "There are three modules we'll need in addition to python are:\n",
    "1.) BeautifulSoup, which you can download by typing: pip install beautifulsoup4 or conda install beautifulsoup4 (for the Anaconda distrbution of Python) in your command prompt.\n",
    "\n",
    "2.) lxml , which you can download by typing: pip install lxml or conda install lxml (for the Anaconda distrbution of Python) in your command prompt.\n",
    "\n",
    "3.) requests, which you can download by typing: pip install requests or conda install requests (for the Anaconda distrbution of Python) in your command prompt.\n",
    "\n",
    "We'll start with our imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7c3fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b26c0d",
   "metadata": {},
   "source": [
    "For our quick web scraping tutorial, we'll look at some legislative reports from the University of California Web Page. Feel free to experiment with other webpages, but remember to be cautious and respectful in what you scrape and how often you do it. Always check the legality of a web scraping job.\n",
    "\n",
    "Let's go ahead and set the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15596f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.ucop.edu/operating-budget/budgets-and-reports/legislative-reports/2013-14-legislative-session.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64849c3",
   "metadata": {},
   "source": [
    "\n",
    "Now let's go ahead and set up requests to grab content form the url, and set it as a Beautiful Soup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3550c218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request content from web page\n",
    "result = requests.get(url)\n",
    "c = result.content\n",
    "\n",
    "# Set as Beautiful Soup Object\n",
    "soup = BeautifulSoup(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebaafba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to the section of interest\n",
    "summary = soup.find(\"div\",{'class':'list-land','id':'content'})\n",
    "\n",
    "# Find the tables in the HTML\n",
    "tables = summary.find_all('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bea29c",
   "metadata": {},
   "source": [
    "Now we need to use Beautiful Soup to find the table entries. A 'td' tag defines a standard cell in an HTML table. The 'tr' tag defines a row in an HTML table.\n",
    "\n",
    "We'll parse through our tables object and try to find each cell using the findALL('td') method.\n",
    "\n",
    "There are tons of options to use with findALL in beautiful soup. You can read about them here http://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bffcee7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Set rows as first indexed object in tables with a row\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m rows \u001b[38;5;241m=\u001b[39m tables[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfindAll(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# now grab every HTML cell in every row\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tr \u001b[38;5;129;01min\u001b[39;00m rows:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Set up empty data list\n",
    "data = []\n",
    "\n",
    "# Set rows as first indexed object in tables with a row\n",
    "rows = tables[0].findAll('tr')\n",
    "\n",
    "# now grab every HTML cell in every row\n",
    "for tr in rows:\n",
    "    cols = tr.findAll('td')\n",
    "    # Check to see if text is in the row\n",
    "    for td in cols:\n",
    "        text = td.find(text=True) \n",
    "        print(text),\n",
    "        data.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b062a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e161063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up empty lists\n",
    "reports = []\n",
    "date = []\n",
    "\n",
    "# Se tindex counter\n",
    "index = 0\n",
    "\n",
    "# Go find the pdf cells\n",
    "for item in data:\n",
    "    if 'pdf' in item:\n",
    "        # Add the date and reports\n",
    "        date.append(data[index-1])\n",
    "        \n",
    "        # Get rid of \\xa0\n",
    "        reports.append(item.replace(u'\\xa0', u' '))\n",
    "                    \n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b917f18",
   "metadata": {},
   "source": [
    "You'll notice a line to take care of '\\xa0 ' This is due to a unicode error that occurs if you don't do this. Web pages can be messy and inconsistent and it is very likely you'll have to do some research to take care of problems like these.\n",
    "\n",
    "Here's the link I used to solve this particular issue: https://stackoverflow.com/questions/10993612/how-to-remove-xa0-from-string-in-python\n",
    "\n",
    "Now all that is left is to organize our data into a pandas DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00445c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Dates and Reports as Series\n",
    "date = Series(date)\n",
    "reports = Series(reports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0dc5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate into a DataFrame\n",
    "legislative_df = pd.concat([date,reports],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8661aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the columns\n",
    "legislative_df.columns = ['Date','Reports']\n",
    "# Show the finished DataFrame\n",
    "legislative_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a90811",
   "metadata": {},
   "source": [
    "There are other less intense options for web scraping:\n",
    "\n",
    "Check out these two companies:\n",
    "\n",
    "https://import.io/\n",
    "\n",
    "https://www.kimonolabs.com/\n",
    "\n",
    "Good Job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef3a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd3beef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9603a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd768b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd63525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623d3d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8853d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ced9a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a18bb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896c543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa459895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aadc6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2865e20c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e82a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aee58f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ead2db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c2464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e47e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8214032f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246bfa81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c778e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5800e74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587751eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b98ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f0981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e9528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f59af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed1a05d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb99f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733fac90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a7ef7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598445a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f67de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ad7f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9411941e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225c9ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e68ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414df493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe03965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f5ff51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7f7366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dade4ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bd280d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5972d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e957e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618cf689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d9467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989f1459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089631e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49b16e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db86e1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be30bdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4cec51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bab3a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28dc1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075328aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd65489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e13d75d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d46f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f0a79b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927f5dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053afac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
